{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducible Framework for Synthetic Clinical Query Generation\n",
    "\n",
    "This notebook provides a **customizable, reproducible pipeline** for generating synthetic clinical queries with ground-truth PHI tags using Azure OpenAI (GPT-4o).\n",
    "\n",
    "## Purpose\n",
    "- **Benchmark Dataset**: For evaluating HIPAA-compliant deidentification systems in AI-clinical workflows\n",
    "- **Customizable Framework**: Adapt query generation for different clinical contexts (academic/community/telemedicine) by modifying few-shot examples\n",
    "- **Reproducible**: Environment-based configuration, pinned dependencies, seeded randomness\n",
    "\n",
    "## Key Features\n",
    "- **Few-shot driven**: LLM behavior is primarily controlled by the three template examples in the prompt\n",
    "- **Append-only**: Safely adds new queries without regenerating existing data\n",
    "- **Validation**: Built-in dataset quality checks post-generation\n",
    "\n",
    "## Requirements\n",
    "- Python 3.12+\n",
    "- Azure OpenAI credentials (set via environment variables)\n",
    "- See `requirements.txt` for dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create requirements.txt if it doesn't exist\n",
    "import os\n",
    "\n",
    "requirements_content = \"\"\"openai==1.75.0\n",
    "tqdm==4.67.1\n",
    "\"\"\"\n",
    "\n",
    "requirements_path = './requirements.txt'\n",
    "if not os.path.exists(requirements_path):\n",
    "    with open(requirements_path, 'w') as f:\n",
    "        f.write(requirements_content)\n",
    "    print(f\"Created {requirements_path}\")\n",
    "else:\n",
    "    print(f\"{requirements_path} already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies from requirements.txt\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import random\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "from openai import AzureOpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed for reproducibility (minimal randomness in this workflow, but ensures consistency)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Configuration: Azure OpenAI Credentials and Settings\n",
    "# ============================================================================\n",
    "# Security: Use environment variables for all sensitive credentials.\n",
    "# Set these before running:\n",
    "#   export AZURE_OPENAI_API_KEY_4o='your_api_key'\n",
    "#   export AZURE_OPENAI_ENDPOINT_4o='your_endpoint'\n",
    "#   export AZURE_OPENAI_DEPLOYMENT_4o='your_deployment_name'\n",
    "# Optional: export AZURE_CONFIG_PATH='./config.ini' if using config file\n",
    "\n",
    "# Try loading from config file if AZURE_CONFIG_PATH is set\n",
    "config_path = os.getenv('AZURE_CONFIG_PATH', './config.ini')\n",
    "\n",
    "if os.path.exists(config_path):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_path)\n",
    "    \n",
    "    AZURE_OPENAI_API_KEY = config.get('AZUREOPENAI', 'API_KEY_4o', fallback=None)\n",
    "    AZURE_OPENAI_ENDPOINT = config.get('AZUREOPENAI', 'ENDPOINT_4o', fallback=None)\n",
    "    AZURE_OPENAI_DEPLOYMENT = config.get('AZUREOPENAI', 'LLM_DEPLOYMENT_NAME_4o', fallback=None)\n",
    "else:\n",
    "    AZURE_OPENAI_API_KEY = None\n",
    "    AZURE_OPENAI_ENDPOINT = None\n",
    "    AZURE_OPENAI_DEPLOYMENT = None\n",
    "\n",
    "# Override with environment variables if set (env vars take precedence)\n",
    "AZURE_OPENAI_API_KEY = os.getenv('AZURE_OPENAI_API_KEY_4o', AZURE_OPENAI_API_KEY)\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv('AZURE_OPENAI_ENDPOINT_4o', AZURE_OPENAI_ENDPOINT)\n",
    "AZURE_OPENAI_DEPLOYMENT = os.getenv('AZURE_OPENAI_DEPLOYMENT_4o', AZURE_OPENAI_DEPLOYMENT)\n",
    "\n",
    "# Validate that all required credentials are present\n",
    "if not all([AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_DEPLOYMENT]):\n",
    "    raise ValueError(\n",
    "        \"Missing required Azure OpenAI credentials. Please set environment variables:\\n\"\n",
    "        \"  AZURE_OPENAI_API_KEY_4o\\n\"\n",
    "        \"  AZURE_OPENAI_ENDPOINT_4o\\n\"\n",
    "        \"  AZURE_OPENAI_DEPLOYMENT_4o\\n\"\n",
    "        \"Or provide a config file at AZURE_CONFIG_PATH\"\n",
    "    )\n",
    "\n",
    "# Output path (points to the actual cleaned dataset by default)\n",
    "OUTPUT_PATH = os.getenv('OUTPUT_PATH', '/Users/jacweath/Desktop/safesearch_/JMIR AI/Synth Data Gen/synthetic_dataset.txt')\n",
    "\n",
    "# Initialize Azure OpenAI client\n",
    "API_VERSION = \"2023-07-01-preview\"\n",
    "TEMPERATURE = 0.9  # Higher temperature for diverse query generation\n",
    "BATCH_SIZE = 5     # Queries generated per API call\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT\n",
    ")\n",
    "\n",
    "# Print generation settings for reproducibility documentation\n",
    "print(\"=\" * 60)\n",
    "print(\"GENERATION SETTINGS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: GPT-4o\")\n",
    "print(f\"Deployment: {AZURE_OPENAI_DEPLOYMENT}\")\n",
    "print(f\"API Version: {API_VERSION}\")\n",
    "print(f\"Temperature: {TEMPERATURE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE} queries per request\")\n",
    "print(f\"Output Path: {OUTPUT_PATH}\")\n",
    "print(f\"Random Seed: 42\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# System Prompt: Defines the synthetic query generation framework\n",
    "# ============================================================================\n",
    "# NOTE: The LLM follows the structure/PHI patterns in the three few-shot \n",
    "# examples (see Examples 1-3 at the bottom of this prompt) more than the \n",
    "# general description. This is intentional: Customize these examples for \n",
    "# domain-specific datasets (e.g., academic vs. community hospital contexts).\n",
    "#\n",
    "# The 'r' prefix marks this as a raw string to prevent SyntaxWarnings from\n",
    "# backslashes used in markdown formatting.\n",
    "\n",
    "phi_query_system_prompt = r\"\"\"\n",
    "### System Prompt for Generation of a Methodologically Robust Synthetic Clinical Query Dataset\n",
    "\n",
    "You are a clinical data simulation assistant. Your mission is to generate a high-quality, benchmark dataset of 500 synthetic clinical search queries. This dataset is designed for the rigorous training and evaluation of automated de-identification pipelines, with a strict focus on HIPAA Safe Harbor standards.\n",
    "\n",
    "The queries must represent conversational interactions where physicians ask AI assistants patient-specific questions requiring evidence-based guidance. In this emerging use case, queries naturally contain PHI because physicians are in the patient's context. The primary objective is to test a system's ability to accurately distinguish and remove specific, legally defined PHI identifiers from clinically rich, unstructured text. The dataset must include a balanced mix of straightforward PHI cases, challenging ambiguous queries, and \"hard negatives\" to robustly measure both detection accuracy and over-redaction risk.\n",
    "\n",
    "The output must be programmatically parsable and adhere to the specified format without deviation.\n",
    "\n",
    "---\n",
    "**1. Query Style and Intent: Information-Seeking vs. Record-Retrieval**\n",
    "\n",
    "This is the most critical instruction. Queries must seek general medical knowledge (e.g., guidelines, evidence, contraindications) using a patient's context, not request a lookup of a patient's specific record.\n",
    "\n",
    "* **Vary your phrasing.** Do not overuse phrases like \"similar to my patient\" or \"a case like.\" Frame the queries more directly.\n",
    "\n",
    "* **GENERATE (Information-Seeking Style):** Queries that seek general medical knowledge.\n",
    "    * *Correct Example:* \"Current guidelines for managing Type 2 Diabetes in a 68-year-old male with a history of CKD, John Doe (MRN: 554-32-11)?\"\n",
    "    * *Correct Example:* \"Contraindications for prescribing Paxlovid to a patient on statins like Sarah M., seen at our Boston clinic on May 1st, 2024?\"\n",
    "\n",
    "* **AVOID (Record-Retrieval Style):** Queries that ask the system to look up specific data.\n",
    "    * *Incorrect Example:* \"Pull the lab results for John Doe (MRN: 554-32-11).\"\n",
    "    * *Incorrect Example:* \"What was Sarah M.'s discharge summary from May 1st, 2024?\"\n",
    "\n",
    "---\n",
    "**2. Output Structure**\n",
    "\n",
    "Each entry must follow this exact format:\n",
    "\n",
    "`===QUERY===`\n",
    "`<A single, realistic clinical search query.>`\n",
    "`===PHI_TAGS===`\n",
    "`<A JSON object on a new line for each piece of PHI present in the query. This section must be empty for hard negative queries.>`\n",
    "\n",
    "* The text within the `===QUERY===` block **MUST** be a single, continuous line of text. **DO NOT** use line breaks within the query.\n",
    "* Each `PHI_TAGS` JSON object **MUST** be a complete, valid JSON object on a single line.\n",
    "\n",
    "The JSON tag must have this structure:\n",
    "\n",
    "`{ \"identifier_type\": \"HIPAA_CATEGORY\", \"value\": \"...\" }`\n",
    "\n",
    "---\n",
    "**3. PHI To Be Tagged (Strictly Limited to HIPAA Safe Harbor Identifiers)**\n",
    "\n",
    "You must ONLY tag the following types of information. If an item is not on this list, do not tag it.\n",
    "\n",
    "* **NAME**: Full or last names of individuals (patients, relatives, providers).\n",
    "* **GEOGRAPHIC\\_LOCATION**: All geographic subdivisions smaller than a state, including street address, city, county, precinct, and ZIP code.\n",
    "* **DATE**: All elements of dates (except year) directly related to an individual (e.g., birth date, admission date).\n",
    "* **PHONE\\_NUMBER**: All telephone numbers.\n",
    "* **FAX\\_NUMBER**: All fax numbers.\n",
    "* **EMAIL\\_ADDRESS**: All email addresses.\n",
    "* **SOCIAL\\_SECURITY\\_NUMBER**: All Social Security numbers.\n",
    "* **MEDICAL\\_RECORD\\_NUMBER**: All medical record numbers.\n",
    "* **HEALTH\\_PLAN\\_BENEFICIARY\\_NUMBER**: All health plan or insurance policy numbers.\n",
    "* **ACCOUNT\\_NUMBER**: All financial or other account numbers.\n",
    "* **CERTIFICATE\\_LICENSE\\_NUMBER**: All certificate or license numbers.\n",
    "* **VEHICLE\\_IDENTIFIER**: All vehicle identifiers and serial numbers, including license plates.\n",
    "* **DEVICE\\_IDENTIFIER**: All medical device identifiers and serial numbers.\n",
    "* **URL**: All Web Universal Resource Locators.\n",
    "* **IP\\_ADDRESS**: All Internet Protocol addresses.\n",
    "* **BIOMETRIC\\_IDENTIFIER**: References to biometric data (e.g., fingerprints, retinal scans).\n",
    "* **FULL\\_FACE\\_PHOTO**: References to full-face photographic images.\n",
    "* **UNIQUE\\_IDENTIFIER**: Any other unique identifying number, characteristic, or code (e.g., clinical trial number).\n",
    "\n",
    "---\n",
    "**4. Information to INCLUDE in the Query but NEVER Tag as PHI**\n",
    "\n",
    "This information provides essential clinical context and must be included in queries but should NEVER be tagged as PHI.\n",
    "\n",
    "* **Demographics**: Age, gender, race, or ethnicity (e.g., \"a 72-year-old Black female\").\n",
    "* **Clinical Status**: Diseases, symptoms, diagnoses, treatments, or medications (e.g., \"diagnosed with Alzheimer's disease,\" \"history of stroke\").\n",
    "* **Eponyms, Scores, and Drug Names**: Non-PHI proper nouns like disease names (Parkinson's disease), medical scores (Wells score), clinical trials (GUSTO trial), or drug brand names (Lipitor).\n",
    "* **Relative Dates**: Non-specific dates or timeframes (e.g., \"last month,\" \"yesterday\").\n",
    "* **Years**: Standalone years are permissible and should not be tagged (e.g., \"diagnosed in 2022\").\n",
    "\n",
    "---\n",
    "**5. Dataset Generation Requirements**\n",
    "\n",
    "* **Dataset Composition**: Within the 500 queries, generate a mix with the following approximate distribution:\n",
    "    * **Core PHI Queries (~60%)**: Contains at least TWO distinct PHI identifiers from Section 3.\n",
    "    * **Hard Negative & Ambiguous Queries (~25%)**: Intentionally designed to resemble PHI but containing none. See Section 6.\n",
    "    * **Complex Syntax Queries (~15%)**: Incorporate realistic shorthand, typos, and varied phrasing. These can be combined with the other two categories.\n",
    "* **Clinical Intent Variety**: Vary the clinical intent to include:\n",
    "    * Prognostic questions (\"What is the 5-year survival rate for...\").\n",
    "    * Drug-specific queries (\"Contraindications for metformin in patients with...\").\n",
    "    * Guideline-seeking (\"Latest ACC/AHA guidelines for...\").\n",
    "    * Comparative questions (\"Effectiveness of apixaban vs. warfarin for...\").\n",
    "    * Lab interpretation (\"Differential diagnosis for elevated AST/ALT in...\").\n",
    "* **Syntactic & Linguistic Variety**: Employ a mix of full natural language questions, clinical shorthand (e.g., \"pt\", \"w/\", \"hx of\", \"dx'd\", \"s/p\", \"yo\" for \"year-old\"), elliptical phrasing (\"post-op fever day 3\"), and occasional common typographical errors (\"side efects of amlodipine\"). Crucially, do not simply copy the structure of the provided examples; they are guides for style, not for sentence construction.\n",
    "\n",
    "---\n",
    "**6. Hard Negative and Ambiguous Case Generation**\n",
    "\n",
    "To robustly test for over-redaction (false positives), roughly 25% of the queries must be \"hard negatives.\" These queries must contain information that could be confused with PHI but is not, and therefore must have an empty `===PHI_TAGS===` section.\n",
    "\n",
    "* **Use Eponyms and Scores**: Include non-PHI proper nouns (e.g., \"Parkinson's disease\", \"Chaddock reflex\", \"Wells score\").\n",
    "* **Use Ambiguous Numbers**: Include dosages, lab values, or non-specific identifiers that are not PHI (e.g., \"bilirubin level of 12\", \"notes from bay 3\", \"protocol for room 12\", \"trial ID 1999-05\").\n",
    "* **Use Vague Identifiers**: Include phrases that are close to but not explicit PHI (e.g., \"patient from the Galveston area,\" \"notes from last summer,\" \"his brother, an engineer\").\n",
    "\n",
    "---\n",
    "**7. Correctly Formatted Examples**\n",
    "\n",
    "*Example 1: Core PHI Query (Direct Style)*\n",
    "`===QUERY===`\n",
    "Latest NCCN guidelines for HER2+ breast cancer in a 45yo female, Jane Doe, whose case was reviewed at Mercy Hospital on June 5th, 2024 (MRN: #JH-876543)?\n",
    "`===PHI_TAGS===`\n",
    "`{ \"identifier_type\": \"NAME\", \"value\": \"Jane Doe\" }`\n",
    "`{ \"identifier_type\": \"GEOGRAPHIC_LOCATION\", \"value\": \"Mercy Hospital\" }`\n",
    "`{ \"identifier_type\": \"DATE\", \"value\": \"June 5th, 2024\" }`\n",
    "`{ \"identifier_type\": \"MEDICAL_RECORD_NUMBER\", \"value\": \"#JH-876543\" }`\n",
    "\n",
    "*Example 2: Hard Negative Query*\n",
    "`===QUERY===`\n",
    "What is the recommended management for a 78-year-old male with a high Wells score and a history of Parkinson's disease diagnosed in 2022?\n",
    "`===PHI_TAGS===`\n",
    "\n",
    "*Example 3: Complex Syntax & \"Messy\" Query*\n",
    "`===QUERY===`\n",
    "rec. tx for pt w/ hx of T2DM & CKD, see notes for Mark J., seen at Mayo on Mar 3rd 2023. worried about side efects of metformin. his insurance is WX-123456.\n",
    "`===PHI_TAGS===`\n",
    "`{ \"identifier_type\": \"NAME\", \"value\": \"Mark J.\" }`\n",
    "`{ \"identifier_type\": \"GEOGRAPHIC_LOCATION\", \"value\": \"Mayo\" }`\n",
    "`{ \"identifier_type\": \"DATE\", \"value\": \"Mar 3rd 2023\" }`\n",
    "`{ \"identifier_type\": \"HEALTH_PLAN_BENEFICIARY_NUMBER\", \"value\": \"WX-123456\" }`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework Design Note: Few-Shot Example Customization\n",
    "\n",
    "**Key Insight**: The LLM primarily follows the **structure and PHI patterns** demonstrated in the three template examples (Examples 1-3 in the prompt above), rather than the general description text. This is **intentional design** that enables flexible, domain-specific adaptation.\n",
    "\n",
    "### Why This Matters\n",
    "Users can customize query generation for specific clinical contexts by **modifying only the three few-shot examples**, without rewriting the entire prompt. The model learns:\n",
    "- Query complexity and phrasing style\n",
    "- Types of PHI to emphasize (e.g., MRNs vs. locations)\n",
    "- Clinical vocabulary level (academic vs. primary care)\n",
    "- Use of abbreviations and shorthand\n",
    "\n",
    "### Example Customizations\n",
    "\n",
    "**Academic Medical Center Context**:\n",
    "- Use complex terminology (\"HER2+ metastatic adenocarcinoma\")\n",
    "- Reference clinical trials and research protocols\n",
    "- Include subspecialty-specific queries\n",
    "\n",
    "**Community Hospital Context**:\n",
    "- Use simpler language (\"breast cancer that spread\")\n",
    "- Focus on primary care conditions (diabetes, hypertension)\n",
    "- Include common comorbidities and polypharmacy\n",
    "\n",
    "**Telemedicine Context**:\n",
    "- Emphasize email addresses and phone numbers as primary identifiers\n",
    "- Include remote monitoring device IDs\n",
    "- Use more conversational phrasing\n",
    "\n",
    "### Practical Use\n",
    "To adapt this framework for your context:\n",
    "1. Identify your target clinical setting\n",
    "2. Write 3 representative queries with realistic PHI\n",
    "3. Replace Examples 1-3 in the prompt above\n",
    "4. Regenerate dataset with the same code\n",
    "\n",
    "This flexibility enables **domain-specific benchmark generation** without changing the generation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Query Generation Function\n",
    "# ============================================================================\n",
    "\n",
    "def generate_phi_queries(n=50, out_path=None):\n",
    "    \"\"\"\n",
    "    Generates a specified number of synthetic queries and appends them to the output file.\n",
    "    \n",
    "    This function requests queries in batches (more efficient than one-by-one generation)\n",
    "    and appends successfully generated batches immediately to preserve progress.\n",
    "\n",
    "    Args:\n",
    "        n (int): The total number of queries to generate.\n",
    "        out_path (str): The file path to save the generated queries. \n",
    "                        Defaults to OUTPUT_PATH from config.\n",
    "    \n",
    "    Returns:\n",
    "        None. Writes queries to file and prints completion message.\n",
    "    \"\"\"\n",
    "    if out_path is None:\n",
    "        out_path = OUTPUT_PATH\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(os.path.dirname(out_path) or '.', exist_ok=True)\n",
    "    \n",
    "    # Calculate batches\n",
    "    num_batches = (n + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    \n",
    "    failed_batches = 0\n",
    "\n",
    "    for batch_num in tqdm(range(num_batches), desc=\"Generating Query Batches\"):\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=AZURE_OPENAI_DEPLOYMENT,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": phi_query_system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"Generate {BATCH_SIZE} new, unique, and realistic clinical queries with structured PHI_TAGS as specified in the system prompt.\"}\n",
    "                ],\n",
    "                temperature=TEMPERATURE,\n",
    "                max_tokens=2500\n",
    "            )\n",
    "            \n",
    "            generated_text = resp.choices[0].message.content\n",
    "            \n",
    "            # Basic validation: Check if response contains expected format markers\n",
    "            if generated_text and '===QUERY===' in generated_text:\n",
    "                # Append to the file immediately after a successful call to save progress\n",
    "                with open(out_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(generated_text.strip() + \"\\n\\n\")\n",
    "            else:\n",
    "                print(f\"\\nWarning: Batch {batch_num+1} returned malformed output; skipping.\")\n",
    "                failed_batches += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            # Log errors to a separate file to not pollute the dataset\n",
    "            error_log_path = \"generation_errors.log\"\n",
    "            with open(error_log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(f\"[{datetime.now()}] Batch {batch_num+1} failed: {e}\\n\")\n",
    "            print(f\"\\nError in batch {batch_num+1}; skipping. See generation_errors.log\")\n",
    "            failed_batches += 1\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Generation complete!\")\n",
    "    print(f\"Queries appended to: {out_path}\")\n",
    "    print(f\"Successful batches: {num_batches - failed_batches}/{num_batches}\")\n",
    "    if failed_batches > 0:\n",
    "        print(f\"Failed batches: {failed_batches} (see generation_errors.log)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Run validation on the output file\n",
    "    if os.path.exists(out_path):\n",
    "        print(\"\\nValidating generated dataset...\")\n",
    "        validate_dataset(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Dataset Validation Function\n",
    "# ============================================================================\n",
    "\n",
    "def validate_dataset(filepath):\n",
    "    \"\"\"\n",
    "    Validate existing dataset: Count queries, PHI elements, and hard negatives.\n",
    "    \n",
    "    This function performs post-generation quality checks to ensure the dataset\n",
    "    meets expected standards for balance and completeness.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Path to the generated synthetic_dataset.txt file\n",
    "    \n",
    "    Returns:\n",
    "        dict: Validation statistics\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: File not found at {filepath}\")\n",
    "        return None\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Split by query markers\n",
    "    queries = content.split('===QUERY===')[1:]  # Skip first empty split\n",
    "    \n",
    "    valid_queries = 0\n",
    "    queries_with_phi = 0\n",
    "    hard_negatives = 0\n",
    "    total_phi_elements = 0\n",
    "    malformed_queries = 0\n",
    "    \n",
    "    for block in queries:\n",
    "        if '===PHI_TAGS===' not in block:\n",
    "            malformed_queries += 1\n",
    "            continue\n",
    "        \n",
    "        query_text, phi_section = block.split('===PHI_TAGS===', 1)\n",
    "        \n",
    "        if query_text.strip():\n",
    "            valid_queries += 1\n",
    "            \n",
    "            # Count PHI elements (lines starting with '{' are JSON tags)\n",
    "            phi_lines = [l for l in phi_section.split('\\n') \n",
    "                        if l.strip() and l.strip().startswith('{')]\n",
    "            \n",
    "            if phi_lines:\n",
    "                queries_with_phi += 1\n",
    "                total_phi_elements += len(phi_lines)\n",
    "            else:\n",
    "                hard_negatives += 1\n",
    "    \n",
    "    # Calculate statistics\n",
    "    phi_percentage = (queries_with_phi / valid_queries * 100) if valid_queries > 0 else 0\n",
    "    hard_neg_percentage = (hard_negatives / valid_queries * 100) if valid_queries > 0 else 0\n",
    "    mean_phi_per_query = (total_phi_elements / valid_queries) if valid_queries > 0 else 0\n",
    "    \n",
    "    # Print validation report\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DATASET VALIDATION REPORT\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total Valid Queries:      {valid_queries}\")\n",
    "    print(f\"Queries with PHI:         {queries_with_phi} ({phi_percentage:.1f}%)\")\n",
    "    print(f\"Hard Negatives (no PHI):  {hard_negatives} ({hard_neg_percentage:.1f}%)\")\n",
    "    print(f\"Total PHI Elements:       {total_phi_elements}\")\n",
    "    print(f\"Mean PHI per Query:       {mean_phi_per_query:.2f}\")\n",
    "    \n",
    "    if malformed_queries > 0:\n",
    "        print(f\"\\n  Malformed Queries:      {malformed_queries}\")\n",
    "    \n",
    "    # Quality checks\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"QUALITY CHECKS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    checks_passed = True\n",
    "    \n",
    "    # Check 1: Hard negatives should be ~25%\n",
    "    if 15 <= hard_neg_percentage <= 35:\n",
    "        print(\"✓ Hard negative ratio within expected range (15-35%)\")\n",
    "    else:\n",
    "        print(f\"  Hard negative ratio ({hard_neg_percentage:.1f}%) outside expected range (15-35%)\")\n",
    "        checks_passed = False\n",
    "    \n",
    "    # Check 2: Mean PHI should be reasonable (2-4 per query)\n",
    "    if 1.5 <= mean_phi_per_query <= 5:\n",
    "        print(\"✓ Mean PHI per query within expected range (1.5-5)\")\n",
    "    else:\n",
    "        print(f\"  Mean PHI per query ({mean_phi_per_query:.2f}) outside expected range (1.5-5)\")\n",
    "        checks_passed = False\n",
    "    \n",
    "    # Check 3: No excessive malformed queries\n",
    "    if malformed_queries < valid_queries * 0.05:  # Less than 5%\n",
    "        print(\"✓ Malformed query rate acceptable (<5%)\")\n",
    "    else:\n",
    "        print(f\"  High malformed query rate: {malformed_queries} ({malformed_queries/(valid_queries+malformed_queries)*100:.1f}%)\")\n",
    "        checks_passed = False\n",
    "    \n",
    "    if checks_passed:\n",
    "        print(f\"\\n✓ All quality checks passed!\")\n",
    "    else:\n",
    "        print(f\"\\n  Some quality checks failed. Review dataset or regenerate.\")\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'valid_queries': valid_queries,\n",
    "        'queries_with_phi': queries_with_phi,\n",
    "        'hard_negatives': hard_negatives,\n",
    "        'total_phi_elements': total_phi_elements,\n",
    "        'mean_phi_per_query': mean_phi_per_query,\n",
    "        'malformed_queries': malformed_queries\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Example Execution: Generate Queries\n",
    "# ============================================================================\n",
    "# This cell demonstrates generating 1000 queries in batches of 5.\n",
    "# For maximum reproducibility, set temperature=0 for deterministic outputs\n",
    "# (though this reduces query diversity).\n",
    "#\n",
    "# The function will:\n",
    "#   1. Generate queries in 200 batches\n",
    "#   2. Append each batch immediately to preserve progress\n",
    "#   3. Log any errors to generation_errors.log\n",
    "#   4. Validate the final dataset automatically\n",
    "#\n",
    "# IMPORTANT: This appends to the output file. If you want to start fresh,\n",
    "# delete or rename the existing synthetic_dataset.txt file first.\n",
    "\n",
    "# Uncomment to run:\n",
    "# generate_phi_queries(n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Validate Existing Dataset\n",
    "# ============================================================================\n",
    "# Run this cell to validate an existing dataset without regenerating queries.\n",
    "# Useful for checking dataset quality after generation or modifications.\n",
    "\n",
    "# Example: Validate the default output file\n",
    "# validate_dataset('./synthetic_dataset.txt')\n",
    "\n",
    "# Or validate a specific file:\n",
    "# validate_dataset('/Users/jacweath/Desktop/safesearch_/JMIR AI/Synth Data Gen/synthetic_dataset.txt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
